{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import random \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/YangyangFu/transformer-time-series@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tsl.transformers.informer import DataLoader\n",
    "from tsl.transformers.patch import PatchTST\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_seed(2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install subversion\n",
    "!svn checkout https://github.com/YangyangFu/transformer-time-series/trunk/datasets\n",
    "!bash ./datasets/download_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example settings \n",
    "embed_dim = 16\n",
    "source_seq_len = 512\n",
    "target_seq_len = 96\n",
    "pred_len = 96\n",
    "target_cols=['OT'] # ['HUFL','HULL','MUFL','MULL','LUFL','LULL','OT']\n",
    "n_targets = len(target_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data path\n",
    "file_path = os.path.dirname(os.path.abspath(__file__))\n",
    "root_path = os.path.dirname(file_path)\n",
    "data_path = os.path.join(root_path, \"datasets\", \"ETT-small\", \"ETTh1.csv\")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(data_path=data_path,\n",
    "                    target_cols=target_cols,\n",
    "                    num_cov_cols=['OT'],\n",
    "                    train_range=(0, 12*30*24),\n",
    "                    val_range=(12*30*24, 12*30*24+4*30*24),\n",
    "                    test_range=(12*30*24+4*30*24, 12*30*24+8*30*24),\n",
    "                    hist_len=source_seq_len,\n",
    "                    token_len=target_seq_len-pred_len,\n",
    "                    pred_len=pred_len,\n",
    "                    batch_size=64,\n",
    "                    )\n",
    "train_ds = dataloader.generate_dataset(mode=\"train\", shuffle=True, seed=1)\n",
    "val_ds = dataloader.generate_dataset(mode=\"validation\", shuffle=False, seed=1)\n",
    "test_ds = dataloader.generate_dataset(mode=\"test\", shuffle=False, seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create informer model\n",
    "target_cols_index = dataloader.target_cols_index\n",
    "\n",
    "model = PatchTST(pred_len = pred_len,\n",
    "                target_cols_index = target_cols_index,\n",
    "                embedding_dim = embed_dim,\n",
    "                num_layers = 3,\n",
    "                num_heads = 4,\n",
    "                ffn_hidden_dim = 128,\n",
    "                patch_size = 16,\n",
    "                patch_strides = 8, \n",
    "                patch_padding = \"end\", \n",
    "                dropout_rate = 0.3,\n",
    "                linear_head_dropout_rate=0.0)\n",
    "\n",
    "# training settings\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "train_metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "val_metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "test_metrics = [tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# train step\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # update metrics\n",
    "    for metric in train_metrics:\n",
    "            metric.update_state(y, y_pred)\n",
    "    return loss\n",
    "\n",
    "# validation step\n",
    "@tf.function\n",
    "def val_step(x, y):\n",
    "    y_pred = model(x, training=False)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    for metric in val_metrics:\n",
    "        metric.update_state(y, y_pred)\n",
    "    return loss\n",
    "\n",
    "# test step\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    y_pred = model(x, training=False)\n",
    "    for metric in test_metrics:\n",
    "        metric.update_state(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "MAX_EPOCHS = 100\n",
    "patience = 3\n",
    "wait = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # take a batch\n",
    "    for batch in train_ds:\n",
    "        x, cat_covs, time_enc, time_dec, target_dec = batch\n",
    "        loss = train_step(x, target_dec[:, -pred_len:, :])\n",
    "        \n",
    "    # print loss every epoch\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} training loss: {loss:.4f}, MAE: {train_metrics[0].result():.4f}\")\n",
    "    \n",
    "    # reset train metrics\n",
    "    for metric in train_metrics:\n",
    "        metric.reset_states()\n",
    "    \n",
    "    # run validation loop\n",
    "    # how to run validaiton loop without batching?\n",
    "    \n",
    "    for val_batch in val_ds:\n",
    "        x, cat_covs, time_enc, time_dec, target_dec = val_batch\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = val_step(x, target_dec[:, -pred_len:, :])\n",
    "        \n",
    "        # print loss every epoch\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} validation loss: {loss_val:.4f}, MAE: {val_metrics[0].result():.4f}\")\n",
    "    \n",
    "    # reset val metrics\n",
    "    for metric in val_metrics:\n",
    "        metric.reset_states()\n",
    "    \n",
    "    ## early stopping\n",
    "    wait += 1\n",
    "    if loss_val < best_val_loss:\n",
    "        best_val_loss = loss_val\n",
    "        wait = 0\n",
    "        model.save_weights(\"patchtst.h5\")\n",
    "    if wait > patience:\n",
    "        print('early stopping...')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test loop     \n",
    "for test_batch in test_ds:\n",
    "    x, cat_covs, time_enc, time_dec, target_dec = test_batch\n",
    "    test_step(x, target_dec[:, -pred_len:, :])\n",
    "\n",
    "# print loss every epoch\n",
    "print(f\"Test loss MSE: {test_metrics[0].result():.4f}, MAE: {test_metrics[1].result():.4f}\")\n",
    "    \n",
    "# reset val metrics\n",
    "for metric in test_metrics:\n",
    "    metric.reset_states()\n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
