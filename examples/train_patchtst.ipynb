{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import random \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/YangyangFu/transformer-time-series@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tsl.dataloader.batch_on_time  import DataLoader\n",
    "from tsl.transformers.patch import PatchTST\n",
    "from tsl.utils.utils import seed_everything\n",
    "\n",
    "seed_everything(2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install subversion\n",
    "!svn checkout https://github.com/YangyangFu/transformer-time-series/trunk/datasets\n",
    "!bash ./datasets/download_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example settings \n",
    "embed_dim = 16\n",
    "source_seq_len = 336\n",
    "target_seq_len = 96\n",
    "pred_len = 96\n",
    "target_cols=['HUFL','HULL','MUFL','MULL','LUFL','LULL','OT']\n",
    "n_targets = len(target_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data path\n",
    "file_path = os.path.dirname(os.path.abspath(__file__))\n",
    "root_path = os.path.dirname(file_path)\n",
    "data_path = os.path.join(root_path, \"datasets\", \"ETT-small\", \"ETTh1.csv\")\n",
    "\n",
    "ts_file = \"ts.joblib\"\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(\n",
    "        data_path,\n",
    "        ts_file,\n",
    "        num_cov_global_file=None,\n",
    "        cat_cov_global_file=None,\n",
    "        num_cov_local_variant_file=[],\n",
    "        cat_cov_local_variant_file=[],\n",
    "        num_cov_local_invariant_file=[],\n",
    "        cat_cov_local_invariant_file=[],\n",
    "        num_cov_local_variant_names=[],\n",
    "        cat_cov_local_variant_names=[],\n",
    "        target_cols=target_cols,\n",
    "        train_range=(0, 24*30*12),\n",
    "        val_range=(24*30*12, 24*30*16),\n",
    "        test_range=(24*30*16, 24*30*20),\n",
    "        hist_len=source_seq_len,\n",
    "        token_len=target_seq_len-pred_len,\n",
    "        pred_len=pred_len,\n",
    "        batch_size=128,\n",
    "        freq='H',\n",
    "        normalize=True,\n",
    "        use_time_features=False,\n",
    "        use_holiday=False,\n",
    "        use_holiday_distance=False,\n",
    "        normalize_time_features=False,\n",
    "        use_history_for_covariates=False\n",
    ")\n",
    "\n",
    "train_ds = dataloader.generate_dataset(mode=\"train\", shuffle=True, seed=1)\n",
    "val_ds = dataloader.generate_dataset(mode=\"validation\", shuffle=False, seed=1)\n",
    "test_ds = dataloader.generate_dataset(mode=\"test\", shuffle=False, seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create informer model\n",
    "target_cols_index = dataloader.target_cols_index\n",
    "\n",
    "model = PatchTST(pred_len = pred_len,\n",
    "                target_cols_index = target_cols_index,\n",
    "                embedding_dim = embed_dim,\n",
    "                num_layers = 3,\n",
    "                num_heads = 4,\n",
    "                ffn_hidden_dim = 128,\n",
    "                patch_size = 16,\n",
    "                patch_strides = 8, \n",
    "                patch_padding = \"end\", \n",
    "                dropout_rate = 0.3,\n",
    "                linear_head_dropout_rate=0.0)\n",
    "\n",
    "# training settings\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "train_metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "val_metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "test_metrics = [tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# train step\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # update metrics\n",
    "    for metric in train_metrics:\n",
    "            metric.update_state(y, y_pred)\n",
    "    return loss\n",
    "\n",
    "# validation step\n",
    "@tf.function\n",
    "def val_step(x, y):\n",
    "    y_pred = model(x, training=False)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    for metric in val_metrics:\n",
    "        metric.update_state(y, y_pred)\n",
    "    return loss\n",
    "\n",
    "# test step\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    y_pred = model(x, training=False)\n",
    "    for metric in test_metrics:\n",
    "        metric.update_state(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "MAX_EPOCHS = 100\n",
    "patience = 3\n",
    "wait = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # take a batch\n",
    "    for batch in train_ds:\n",
    "        enc, dec = batch\n",
    "        (ts_enc, num_global_enc, cat_global_enc, \n",
    "         num_local_variant_enc, cat_local_variant_enc, \n",
    "         num_local_invariant_enc, cat_local_invariant_enc, time_features_enc) = enc\n",
    "        (ts_dec, num_global_dec, cat_global_dec, \n",
    "         num_local_variant_dec, cat_local_variant_dec, \n",
    "         num_local_invariant_dec, cat_local_invariant_dec, time_features_dec) = dec\n",
    "        \n",
    "        loss = train_step(ts_enc, ts_dec[:, -pred_len:, :])\n",
    "        \n",
    "    # print loss every epoch\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} training loss: {loss:.4f}, MAE: {train_metrics[0].result():.4f}\")\n",
    "    \n",
    "    # reset train metrics\n",
    "    for metric in train_metrics:\n",
    "        metric.reset_states()\n",
    "    \n",
    "    # run validation loop\n",
    "    # how to run validaiton loop without batching?\n",
    "    \n",
    "    for val_batch in val_ds:\n",
    "        enc, dec = val_batch\n",
    "        (ts_enc, num_global_enc, cat_global_enc, \n",
    "         num_local_variant_enc, cat_local_variant_enc, \n",
    "         num_local_invariant_enc, cat_local_invariant_enc, time_features_enc) = enc\n",
    "        (ts_dec, num_global_dec, cat_global_dec, \n",
    "         num_local_variant_dec, cat_local_variant_dec, \n",
    "         num_local_invariant_dec, cat_local_invariant_dec, time_features_dec) = dec\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = val_step(ts_enc, ts_dec[:, -pred_len:, :])\n",
    "        \n",
    "        # print loss every epoch\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} validation loss: {loss_val:.4f}, MAE: {val_metrics[0].result():.4f}\")\n",
    "    \n",
    "    # reset val metrics\n",
    "    for metric in val_metrics:\n",
    "        metric.reset_states()\n",
    "    \n",
    "    ## early stopping\n",
    "    wait += 1\n",
    "    if loss_val < best_val_loss:\n",
    "        best_val_loss = loss_val\n",
    "        wait = 0\n",
    "        model.save_weights(\"patchtst.h5\")\n",
    "    if wait > patience:\n",
    "        print('early stopping...')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test loop     \n",
    "for test_batch in test_ds:\n",
    "    enc, dec = test_batch\n",
    "    (ts_enc, num_global_enc, cat_global_enc, \n",
    "        num_local_variant_enc, cat_local_variant_enc, \n",
    "        num_local_invariant_enc, cat_local_invariant_enc, time_features_enc) = enc\n",
    "    (ts_dec, num_global_dec, cat_global_dec, \n",
    "        num_local_variant_dec, cat_local_variant_dec, \n",
    "        num_local_invariant_dec, cat_local_invariant_dec, time_features_dec) = dec\n",
    "        \n",
    "    test_step(ts_enc, ts_dec[:, -pred_len:, :])\n",
    "    \n",
    "# print loss every epoch\n",
    "print(f\"Test loss MSE: {test_metrics[0].result():.4f}, MAE: {test_metrics[1].result():.4f}\")\n",
    "    \n",
    "# reset val metrics\n",
    "for metric in test_metrics:\n",
    "    metric.reset_states()\n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
