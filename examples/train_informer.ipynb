{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 23:02:16.512366: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-21 23:02:16.536574: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 23:02:16.937721: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m \n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m__file__\u001b[39;49m))\n\u001b[1;32m      6\u001b[0m root_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(file_path)\n\u001b[1;32m      8\u001b[0m \u001b[39m# add root_path to pythonpath\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os \n",
    "import random \n",
    "import numpy as np\n",
    "file_path = os.path.dirname(os.path.abspath(__file__))\n",
    "root_path = os.path.dirname(file_path)\n",
    "\n",
    "# add root_path to pythonpath\n",
    "import sys\n",
    "os.environ['PYTHONPATH'] = root_path + ':' + os.environ.get('PYTHONPATH', '')\n",
    "print(os.environ['PYTHONPATH'])\n",
    "\n",
    "from tsl.transformers.informer import DataLoader\n",
    "from tsl.transformers.informer import Informer\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example settings \n",
    "embed_dim = 512\n",
    "source_seq_len = 64\n",
    "target_seq_len = 128\n",
    "pred_len = 96\n",
    "n_num_covs = 7\n",
    "n_targets = 1\n",
    "\n",
    "MAX_EPOCHS = 10\n",
    "\n",
    "# get data path\n",
    "data_path = os.path.join(root_path, \"datasets\", \"ETT-small\", \"ETTh1.csv\")\n",
    "\n",
    "# create dataloader\n",
    "dataloader = DataLoader(data_path=data_path,\n",
    "                    target_cols=['OT'],\n",
    "                    num_cov_cols=['HUFL','HULL','MUFL','MULL','LUFL','LULL','OT'],\n",
    "                    train_range=(0, 10000),\n",
    "                    val_range=(10000, 11000),\n",
    "                    test_range=(11000, 12000),\n",
    "                    hist_len=source_seq_len,\n",
    "                    token_len=target_seq_len-pred_len,\n",
    "                    pred_len=pred_len,\n",
    "                    batch_size=32,\n",
    "                    )\n",
    "train_ds = dataloader.generate_dataset(mode=\"train\", shuffle=True, seed=1)\n",
    "val_ds = dataloader.generate_dataset(mode=\"validation\", shuffle=False, seed=1)\n",
    "test_ds = dataloader.generate_dataset(mode=\"test\", shuffle=False, seed=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create informer model\n",
    "model = Informer(output_dim=n_targets, \n",
    "                pred_len=pred_len,\n",
    "                num_layers_encoder=4, \n",
    "                num_heads_encoder=16, \n",
    "                key_dim_encoder=32, \n",
    "                value_dim_encoder=32, \n",
    "                output_dim_encoder=512, \n",
    "                hidden_dim_encoder=2048, \n",
    "                factor_encoder=4,\n",
    "                num_layers_decoder=2, \n",
    "                num_heads_decoder=8, \n",
    "                key_dim_decoder=64, \n",
    "                value_dim_decoder=64, \n",
    "                output_dim_decoder=512, \n",
    "                hidden_dim_decoder=2048, \n",
    "                factor_decoder=4, \n",
    "                num_cat_cov=0,\n",
    "                cat_cov_embedding_size=[],\n",
    "                cat_cov_embedding_dim=16,\n",
    "                freq='H',\n",
    "                use_holiday=True,\n",
    "                dropout_rate=0.1,)\n",
    "\n",
    "# training settings\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "train_metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "val_metrics = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# train step\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_enc, x_dec = x\n",
    "        y_pred = model(x_enc, x_dec, training=True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # update metrics\n",
    "    for metric in train_metrics:\n",
    "            metric.update_state(target_dec[:, -pred_len:, :], y_pred)\n",
    "    return loss\n",
    "\n",
    "# validation step\n",
    "@tf.function\n",
    "def val_step(x, y):\n",
    "    x_enc, x_dec = x\n",
    "    y_pred = model(x_enc, x_dec, training=False)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    for metric in val_metrics:\n",
    "        metric.update_state(target_dec[:, -pred_len:, :], y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # take a batch\n",
    "    for batch in train_ds:\n",
    "        num_covs, cat_covs, time_enc, time_dec, target_dec = batch\n",
    "        \n",
    "        try: \n",
    "            # zero for target \n",
    "            token_dec = target_dec[:, :-pred_len, :]\n",
    "            zeros = tf.zeros_like(target_dec[:, -pred_len:, :])\n",
    "            token_target_dec = tf.concat([token_dec, zeros], axis=1)\n",
    "            \n",
    "            # feed model\n",
    "            x_enc = (num_covs, cat_covs, time_enc)\n",
    "            x_dec = (time_dec, token_target_dec)\n",
    "            \n",
    "            # train step\n",
    "            loss = train_step((x_enc, x_dec), target_dec[:, -pred_len:, :])\n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "    # print loss every epoch\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} training loss: {loss:.4f}, MAE: {train_metrics[0].result():.4f}\")\n",
    "    \n",
    "    # reset train metrics\n",
    "    for metric in train_metrics:\n",
    "        metric.reset_states()\n",
    "    \n",
    "    # run validation loop\n",
    "    # how to run validaiton loop without batching?\n",
    "    \n",
    "    for val_batch in val_ds:\n",
    "        num_covs, cat_covs, time_enc, time_dec, target_dec = val_batch\n",
    "        \n",
    "        try:\n",
    "            # zero for target \n",
    "            token_dec = target_dec[:, :-pred_len, :]\n",
    "            zeros = tf.zeros_like(target_dec[:, -pred_len:, :])\n",
    "            token_target_dec = tf.concat([token_dec, zeros], axis=1)\n",
    "            \n",
    "            # feed model\n",
    "            x_enc = (num_covs, cat_covs, time_enc)\n",
    "            x_dec = (time_dec, token_target_dec)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss_val = val_step((x_enc, x_dec), target_dec[:, -pred_len:, :])\n",
    "        \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "        # print loss every epoch\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} validation loss: {loss_val:.4f}, MAE: {val_metrics[0].result():.4f}\")\n",
    "    \n",
    "    # reset val metrics\n",
    "    for metric in val_metrics:\n",
    "        metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
